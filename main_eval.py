
##################################################################
### IMPORTS
##################################################################

import os,sys,time
from argparse import ArgumentParser
import pickle
import numpy as np
import h5py
from math import sqrt
from itertools import combinations

import torch as to
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
to.set_default_dtype(to.float32)

import util_code.distributions as distributions
import util_code.volume_elements as volume_elements
import util_code.IO as IO
from util_code.networks import RealNVP
from util_code.basic_util import to_matrix_extended, gradient, divergence
from util_code.main_util import init_parallel, init_variables, init_model



##################################################################
### PARAMETERS
##################################################################

## Training parameters
logf_network_file = "nf_results/states/f_state_epoch_80.pt"    # state-file for the previously trained single-point pdf network (see nf_optim files)
G_network_file    = "traj_results/states/G_state_epoch_20.pt"  # state-file from previous training to use for initialization of the MULTI-TIME stat. network
                    # (set to None for random initializition)
out_dir = "eval_results/" # directory to save results in

## Integration parameters
dt = 1e-3                # dt of time-integration
# final saved trajectory in .h5 file will contain N_steps x N_times trajectories
N_part  = 8192       # Number of particles to use per process
N_steps = 2048       # number of timesteps to integrate in a batch
N_times = 20         # number of batches with N_steps each to simulate 
divergence_threshold = 8 # consider a trajectory diverged after any components absolute value is larger than this, reset that traj. to zero
initialization_method = "DNS" # method to use for initializing the simulation available options: 
                              # "DNS", "Gaussian", "restart_sim" (see following attributes), "Lyapunov" 
# only for initialization_method = "restart_sim":
restart_sim_file = None       # .h5 file of the simulation to restart from (must have been generated by this script before)
restart_sim_it = None         # iteration from the file to restart from, following results will be written in the same file after this it


## Model parameters
N_var = 8 
N_up = 1
N_un = N_var-N_up
N_sbins = 8
N_iter_layer = 4
transf_layer = "linear" # alternatively "rational_quadratic"
distribution = distributions.normal ##_01 #normal_01 #normal #strain
# neural network parameters
channels= { # Single-point pdf network not optimized here (only loaded from a previous nf_optim)
            "widths":  [N_un]+[4*N_sbins*N_up]+[N_sbins*N_up], \
            "heights": [N_un]+[4*N_sbins*N_up]+[N_sbins*N_up], \
            "derivatives": [N_un]+[4*(N_sbins+1)*N_up]+[(N_sbins+1)*N_up],\
            "rotation": [N_un]+[64,64]+[N_up], \
            "aff": [N_un]+[64,64]+[N_up], \
            # Multi-time network not optimized here (only loaded from a previous traj_optim)
            "G": [N_var]+[128,128,128]+[N_var**2], \
            "Sigma": [N_var]+[96,96]+[N_var**2]}

## Data loading parameters
# (supports loading of multiple files from the given dir. as long as they are numbered consecutively, 
#  nfile0 sets the first file to start loading from)
N_files = 1       # Number of files to load in a sequence to load longer trajectories
                  # (assuming trajectories are split into multiple files)

data_dir  = "data/" # base file-directory
file_name = "tracked_part1_A_starting_{:03d}.bin" # file-namespace
nfile0    = 30     # ID of the first file to load

N_part_per_file = 262144  # Number of particles saved in each reference data-file
N_steps_per_file   = 500  # Number of timesteps saved in each reference data-file  
dt_data = 1e-3            # dt of reference data 
                          # (should be equal to or a clean fraction of dt above to avoid supersampling artifacts)

##################################################################
### CLASSES AND FUNCTIONS
##################################################################

# Evaluator class for convient initialization and use of class variables 
# to allow access to general variables across different functions during training
class Evaluator():

    def __init__(self, node, ntasks, world_size, ngpus):
        super().__init__()

        self.node = node
        self.ntasks = ntasks
        self.world_size = world_size
        self.ngpus = ngpus
        
        self.N_part  = N_part
        
        self.N_part_per_file = N_part_per_file
        self.N_steps_per_file  = N_steps_per_file

        self.dt = dt
        self.dt_data = dt_data
        self.N_steps = N_steps
        self.N_times = N_times
        self.divergence_threshold = divergence_threshold

        self.N_up = N_up
        self.N_un = N_un
        self.N_sbins = N_sbins
        self.N_iter_layer = N_iter_layer
        self.channels = channels

        self.transf_layer = transf_layer
        self.distribution = distribution

        self.G_network_file = G_network_file
        self.logf_network_file = logf_network_file
        self.out_dir = out_dir

        self.data_dir = data_dir
        self.file_name = file_name
        self.nfile0 = nfile0
        

        self.N_part_file = min(world_size*N_part, N_part_per_file) #131072)
        self.N_part_tot      = N_part*world_size
        self.N_files = max(N_files,self.N_part_tot//self.N_part_file) # ensure enough particles are loaded in any case
        self.nranks_per_file = self.N_part_per_file//N_part

        self.restart_sim_file = restart_sim_file
        self.restart_sim_it   = restart_sim_it
        self.initialization_method = initialization_method

        return


    def evaluate(self, task):
        ##################################################################
        ### INITIALIZATION
        ##################################################################

        init_parallel(self,task)
        init_variables(self)
        init_model(self,task)

        # set manual seed differently for each task here, as we dont need 
        # any synchronized updates required for the training
        to.manual_seed(self.rank)

        if(self.rank==0):
            os.makedirs(self.out_dir,exist_ok=True)


        ##################################################################
        ### LOADING OF REFERENCE DATA
        ##################################################################
        
        fn = self.data_dir+self.file_name.format(self.nfile0+self.rank//self.nranks_per_file)
        A  = IO.read_binary_DNS(fn, self.N_part_per_file, self.N_steps_per_file, self.rank, self.nranks_per_file)

        tau_eta = to.einsum("tsij,tsij->ts", A, A).mean().to(self.device)
        dist.all_reduce(tau_eta, op=dist.ReduceOp.SUM)
        tau_eta/=self.world_size
        tau_eta = to.sqrt(1/tau_eta).to("cpu")
        if self.rank==0:
            print("Kolmogorov time ", tau_eta.item())
        A = A*tau_eta
        s = A.shape
        A = A.reshape(s[0],s[1],9)[:,:,:self.N_var]
        A = A[ :self.N_steps,:self.N_part]



        ##################################################################
        ### INITIAL CONDITIONS FOR TIME INTEGRATION
        ##################################################################   
        
        tau, loss, stats = self.compute_moments(A.to(self.device).float())
        if self.rank==0:
            with open(self.out_dir+"reference"+".txt", "a") as f:
                np.savetxt(f, stats)


        if self.initialization_method == "restart_sim":
            A_ens = to.zeros([self.N_steps, self.N_part, self.N_var], device=self.device)
            A_ens[-1] = (self.read_prev_sim(self.restart_sim_it-1)[-1]).reshape(-1,9)[:,:self.N_var]
            A_ens[-1] = to.where(A_ens[-1]==A_ens[-1], A_ens[-1], to.zeros(A_ens[-1].shape, device=A_ens.device, requires_grad=False))
        
        elif self.initialization_method == "Gaussian": 
            A_ens = to.zeros([self.N_steps, self.N_part, 3,3], device=self.device)
            A_ens[-1] = to.normal(0,1, [self.N_part,3,3], device=self.device).detach()
            A_ens[-1]-= to.einsum("ij,skk->sij", to.eye(3, device=self.device)/3, A_ens[-1]).detach()
            
            tau = to.einsum("sij,sij->s", A_ens[-1], A_ens[-1]).mean().to(self.device)
            dist.all_reduce(tau, op=dist.ReduceOp.SUM)
            tau/=self.world_size
            tau = to.sqrt(1/tau).to("cpu")
            if self.rank==0:
                print("Kolmogorov time ens ", tau.item())
            A_ens = A_ens*tau
            A_ens = A_ens.reshape(-1,self.N_part,9)[:,:,:self.N_var]

        elif self.initialization_method == "DNS": 
            A_ens = to.zeros([self.N_steps,*A.shape[1:]], device=A.device)
            A_ens[-1] = A[0]

        elif self.initialization_method == "Lyapunov": 
            A_ens = to.zeros([self.N_steps, self.N_part, self.N_var], device=A.device)
            s = A.shape
            Ns = s[1]
            A_ens[-1] = A[0]
            A_ens[-1,1::2] = A_ens[-1,0::2] + 1e-4*(to.rand(A[0,:Ns//2].shape, device=A.device).detach()-.5)
            A = to_matrix_extended(A_ens).detach()
            eps0 = to.einsum("sij->s", (A[-1,1::2] - A[-1,0::2])**2).to(self.device)
            if self.rank==0:
                print("Max. initial perturbation ", to.max(eps0).item())

        del A

        A_ens = A_ens.to(self.device)
        sqrt_dt  = float(sqrt(1*self.dt))
        sqrt_2dt = float(sqrt(2*self.dt))


        
        ##################################################################
        ### EVALUATION / TIME INTEGRATION
        ##################################################################

        # if we are initializing from a previous simulation, append the next results at the end of that file
        for write_it in range(0 if self.restart_sim_it == None else self.restart_sim_it,\
                              self.N_times if self.restart_sim_it == None else self.restart_sim_it+self.N_times):

            A_ens[0] = 1*A_ens[-1]

            #write A_ens loop
            time0 = time.time()
            for it in range(self.N_steps-1):
                with to.no_grad():
                    A  = A_ens[it].to(self.device)
                N0 = self.rhs(A)
                with to.no_grad():
                    A1 = A + self.dt*N0
                N1 = self.rhs(A1)
                with to.no_grad():
                    A_ens[it+1] = A + .5*self.dt*(N0 + N1)

                    div_count = to.sum((to.abs(A_ens[it+1])>=self.divergence_threshold).float())
                    dist.reduce(div_count, 0, op=dist.ReduceOp.SUM)
                    A_ens[it+1] = to.where(A_ens[it+1]==A_ens[it+1], A_ens[it+1], to.zeros(A.shape, device=A.device, requires_grad=False))
                    A_ens[it+1] = to.where(to.abs(A_ens[it+1])<self.divergence_threshold, A_ens[it+1], to.zeros(A.shape, device=A.device, requires_grad=False))
                    #
                    if self.rank==0:
                        if div_count>.5:
                            print("Iteration ",it, ":\tDivergence count ", div_count.item())
                            sys.stdout.flush()
                    
                

            dist.barrier()
            stride = 1
            A = to_matrix_extended(A_ens[::4,::stride].clone()).detach()

            # write results to file
            # if we are initializing from a previous simulation, append the next results at the end of that file
            if self.rank==0:
                ###Init h5 file
                with h5py.File(self.out_dir+"/A_ens_{}".format(self.N_part)+".h5", 'a') as f:
                    dset = "A_ens_{}".format(write_it)
                    f.create_dataset(dset, shape=(self.N_steps//4, self.world_size*self.N_part//stride, 3, 3))
                    f[dset][:,0*self.N_part//stride:(0+1)*self.N_part//stride] = A.detach().cpu()
                    for i in range(1,self.world_size):
                        dist.recv(A.contiguous(), i, tag=i)
                        f[dset][:,i*self.N_part//stride:(i+1)*self.N_part//stride] = A.detach().cpu()
            else:
                dist.send(A.contiguous(), dst=0, tag=self.rank)
            dist.barrier()

            tau, loss, stats = self.compute_moments(A_ens)
            if self.rank==0:
                print("Iteration {} ({} s):\tEns. mean {}\tLoss {}\tTau {}".format(write_it, time.time()-time0,to.mean(A_ens[-1]),loss,tau))
                sys.stdout.flush()
                with open(self.out_dir+"ensemble"+".txt", "a") as f:
                    np.savetxt(f, stats)

            if self.initialization_method == "Lyapunov":
                sim_time = (write_it*self.N_steps*self.dt + to.linspace(1,self.N_steps,self.N_steps)*self.dt).to(self.device)
                sim_time = to.maximum(sim_time, self.dt*to.ones(sim_time.shape, device=sim_time.device))
                with to.no_grad():
                    A = to_matrix_extended(A_ens).detach()
                    eps = to.einsum("tsij->ts", (A[:,1::2] - A[:,0::2])**2)

                    lam = (to.log(eps/eps0[None,:]))/(2*sim_time[:,None])

                    lam = lam.detach().to(self.device)
                    lam = to.where(lam==lam, lam, to.zeros(lam.shape, device=lam.device).detach())
                    ###pdf of lam array
                    min_lam = lam.min(); max_lam = lam.max();
                    dist.all_reduce(min_lam, op=dist.ReduceOp.MIN)
                    dist.all_reduce(max_lam, op=dist.ReduceOp.MAX)
                    min_lam = min_lam.detach().cpu().numpy()
                    max_lam = max_lam.detach().cpu().numpy()
                    
                    lam = lam.detach().cpu().numpy()
                    N_bins = 1024
                    lam_pdf, bins = np.histogram(lam.flatten(), range=(min_lam,max_lam), bins=N_bins, density=False)
                    
                    lam_pdf = to.Tensor(lam_pdf).detach().to(self.device) 
                    lam = to.Tensor(lam).detach().to(self.device)
                    dist.reduce(lam_pdf, 0, op=dist.ReduceOp.SUM)
                    
                    lam = lam.mean(1)/self.world_size
                    dist.reduce(lam, 0, op=dist.ReduceOp.SUM)
                    if self.rank==0:
                        lam = lam.cpu().float().numpy()
                        pickle.dump(lam, open((self.out_dir+"Lyapunov_%d" % write_it)+".bin", "wb"))
                        lam_pdf = lam_pdf.cpu().float().numpy()
                        bins = .5*(bins[1:]+bins[:-1])
                        lam_pdf/=(np.sum(lam_pdf)*(bins[1]-bins[0]))
                        pickle.dump([bins, lam_pdf], open((self.out_dir+"Lyapunov_pdf_%d" % write_it)+".bin", "wb"))

        del A_ens, A

        return



    def rhs(self, A):
        A.requires_grad = True
        _, log_prob = self.log_f.log_probability(A, self.transf_layer)
        dF = gradient(log_prob, A)
        G = self.G_model(A).reshape(-1, self.N_var, self.N_var)
        T = G - to.transpose(G, -1, -2)
        N = to.einsum("sij,sj->si", T, dF)
        for i in range(self.N_var):
            N[:,i] += divergence(T[:,i], A)
        return N

    def read_prev_sim(self, it):
        A = to.zeros([self.N_steps, self.N_part, 3, 3], device=self.device).detach()
        with to.no_grad():
            dist.barrier()
            for i in range(self.world_size):
                if self.rank==i:
                    with h5py.File(self.restart_sim_file, 'r') as f:
                        dset = "A_ens_{}".format(it)
                        A = to.tensor(f[dset][:,i*self.N_part:(i+1)*self.N_part]).detach().to(self.device)
                dist.barrier()

            return A
   
    def compute_loss(self, A):
        with to.no_grad():
            _, log_prob = self.log_f.log_probability(A, self.transf_layer)
            loss = -to.mean(log_prob)/self.world_size
            dist.all_reduce(loss, op=dist.ReduceOp.SUM)
        return loss

    def compute_moments(self, A_ens):
        #moments
        A = A_ens.reshape(-1,self.N_var).detach()
        loss = self.compute_loss(A)
        A = to_matrix_extended(A_ens).detach()
        S = .5*(A + to.transpose(A,-1,-2))
        W = (A-S).to(self.device)
        S = S.to(self.device)
        #
        x1 = to.einsum("tsij,tsji->ts", S, S).mean()/self.world_size
        x2 = to.einsum("tsij,tsji->ts", W, W).mean()/self.world_size
        x3 = to.einsum("tsij,tsjk,tski->ts", S, S, S).mean()/self.world_size
        x4 = to.einsum("tsij,tsjk,tski->ts", S, W, W).mean()/self.world_size
        x5 = to.einsum("tsij,tsjk,tskl,tsli->ts", S, S, W, W).mean()/self.world_size
        a2_l = (S[...,0,0]**2).mean()/self.world_size
        a4_l = (S[...,0,0]**4).mean()/self.world_size
        a2_t = ((S[...,0,1]+W[...,0,1])**2).mean()/self.world_size
        a4_t = ((S[...,0,1]+W[...,0,1])**4).mean()/self.world_size
        #
        dist.reduce(x1, 0, op=dist.ReduceOp.SUM)
        dist.reduce(x2, 0, op=dist.ReduceOp.SUM)
        dist.reduce(x3, 0, op=dist.ReduceOp.SUM)
        dist.reduce(x4, 0, op=dist.ReduceOp.SUM)
        dist.reduce(x5, 0, op=dist.ReduceOp.SUM)
        dist.reduce(a2_l, 0, op=dist.ReduceOp.SUM)
        dist.reduce(a4_l, 0, op=dist.ReduceOp.SUM)
        dist.reduce(a2_t, 0, op=dist.ReduceOp.SUM)
        dist.reduce(a4_t, 0, op=dist.ReduceOp.SUM)
        #
        tau = to.sqrt(1/(x1-x2))
        tau = tau.detach().cpu().item()
        #
        x1 = x1.detach().cpu().item()
        x2 = x2.detach().cpu().item()
        x3 = x3.detach().cpu().item()
        x4 = x4.detach().cpu().item()
        x5 = x5.detach().cpu().item()
        a2_l = a2_l.detach().cpu().item()
        a4_l = a4_l.detach().cpu().item()
        a2_t = a2_t.detach().cpu().item()
        a4_t = a4_t.detach().cpu().item()
        loss = loss.detach().cpu().item()
        return tau, loss, np.array([loss,x1,x2,x3,x4,x5,a2_l,a4_l,a2_t,a4_t]).reshape(1,-1)



##################################################################
### MAIN METHOD
##################################################################

if __name__=="__main__":
    #run parameters
    parser = ArgumentParser()
    parser.add_argument("--ntasks", default=1, type=int, help="number of tasks per node (same as number of GPUs if ngpus>0)")
    parser.add_argument("--ngpus",  default=1, type=int, help="number of gpus per node")
    parser.add_argument("--nnodes", default=1, type=int, help="number of nodes")
    parser.add_argument("--ip_address", required=True, type=str, help="IP address of the master node")
    args = parser.parse_args()
    assert args.nnodes > 0
    assert args.ntasks > 0
    if args.ngpus > 0:
        assert to.cuda.is_available()
        assert args.ngpus == args.ntasks
        args.device = "cuda:"

    # are we on a cluster? If not, default node 0
    node = os.environ.get("SLURM_NODEID")
    node = int(node) if node is not None else 0
    # in total ntasks tasks per node
    world_size = args.ntasks*args.nnodes
    assert node < args.nnodes
    print("Here is node", node+1, "/", args.nnodes)
    #set address of the master node
    print("Master ip_address is", args.ip_address)
    os.environ["MASTER_ADRR"] = args.ip_address
    os.environ['MASTER_PORT'] = '8888'
    os.environ['WORLD_SIZE'] = str(world_size)

    E = Evaluator(node, args.ntasks, world_size, args.ngpus)
    mp.spawn(E.evaluate, nprocs=args.ntasks)
